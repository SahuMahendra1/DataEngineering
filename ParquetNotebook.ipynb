{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efaf7933-9671-45ff-a74a-57f06a9bc323",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To understand parquet file formate let's understand row based and column based file formate and why we need them,\n",
    "1. Row based file format: organizes data in a tabular structure, where rows represent individual records and columns represent attributes or fields of those records. These formats are widely used for structured data storage and exchange. Here are some common examples:\n",
    "\n",
    "Name,Age,City\n",
    "Alice,30,Indore\n",
    "Bob,25,Bhopal\n",
    "\n",
    "so, the data store in physical memory is like \n",
    "\n",
    "Alice,30,Indore Bob,25,Bhopal...\n",
    "\n",
    "\n",
    "2. Column based file format: Column-based file formats store data column by column, which means that each columnâ€™s data is stored together. This format is highly efficient for analytical queries that typically retrieve only a subset of columns from large datasets.\n",
    "\n",
    "Parquet\n",
    "\n",
    "Description: A columnar storage file format optimized for use with data processing frameworks like Apache Spark, Hive, and Impala.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Highly efficient for analytical queries\n",
    "\n",
    "Supports complex data types and nested structures\n",
    "\n",
    "Excellent performance with compression\n",
    "\n",
    "* write one and read many format.\n",
    "* based on RLE(run length encoding)\n",
    "* bit packed and use compresssion \n",
    "\n",
    "we need to why we need parquet:\n",
    "\n",
    "Predicate Pushdown\n",
    "Predicate Pushdown is an optimization technique where filter conditions (predicates) are pushed down to the data source level, meaning that the filtering is done as close to the data source as possible, rather than loading all the data into Spark and then filtering it. This reduces the amount of data that needs to be read and processed by Spark, improving performance.\n",
    "In PySpark, predicate pushdown works with various data sources such as Parquet, ORC, and JDBC. For example, if you are querying a Parquet file and you apply a filter condition, Spark can push this filter down to the Parquet file reader. This means only the relevant rows are read from disk, which speeds up the query.\n",
    "\n",
    "Partition Pruning is a technique used to optimize queries on partitioned tables. When data is partitioned (e.g., by date or region), queries that include filter conditions on the partition column can benefit from partition pruning. Partition pruning means Spark will scan only the relevant partitions of the data rather than the entire dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a05beae-bade-4163-b29c-be8ebd332e26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf= spark.read.parquet(\"/Volumes/workspace/default/parqute/part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz (1).parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd8d8142-4c0b-49cf-b01f-4d9af2de62f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(pdf)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ParquetNotebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
