{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca0bd297-371e-43c9-958d-b8fae9e7460a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "A SparkSession is the entry point to programming with Apache Spark. Think of it as your gateway to everything Spark offersâ€”whether you're working with DataFrames, Datasets, SQL queries, or reading from data sources like JSON, Parquet, or Hive.\n",
    "\n",
    "Why SparkSession Matters\n",
    "Before Spark 2.0, you had to juggle multiple contexts:\n",
    "\n",
    "SparkContext for RDDs\n",
    "\n",
    "SQLContext for SQL queries\n",
    "\n",
    "HiveContext for Hive integration\n",
    "\n",
    "With Spark 2.0 and beyond, SparkSession unifies all of these into a single object, simplifying your code and making it easier to manage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62c6d128-f554-4aab-b00b-7e029c57c43e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Stop the existing SparkSession if it exists\n",
    "if 'spark' in locals():\n",
    "    spark.stop()\n",
    "\n",
    "# Create a new SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    # Removed the unsupported configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54c912d4-6160-4133-9f0b-3e1beb69eec2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(spark,\"SparkSession created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SparkContext and SparkSession",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
